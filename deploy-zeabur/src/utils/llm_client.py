# LLMå®¢æˆ·ç«¯ - OpenRouteré›†æˆ
"""
æ™ºèƒ½é™çº§çš„LLMå®¢æˆ·ç«¯
æ”¯æŒOpenRouter APIè°ƒç”¨ï¼Œå¸¦æœ‰æ™ºèƒ½é™çº§å’Œç¼“å­˜æœºåˆ¶
"""

import os
import json
import asyncio
from typing import Dict, Any, Optional, List
from datetime import datetime, timedelta
from dataclasses import dataclass
from loguru import logger
import aiohttp

@dataclass
class LLMResponse:
    """LLMå“åº”ç»“æœ"""
    content: str
    success: bool
    source: str  # "llm" or "fallback"
    tokens_used: int = 0
    response_time: float = 0.0
    error: Optional[str] = None

class LLMClient:
    """æ™ºèƒ½é™çº§çš„LLMå®¢æˆ·ç«¯"""
    
    def __init__(self):
        # æ£€æµ‹ä½¿ç”¨çš„LLMæ–¹æ¡ˆ
        self.anthropic_key = os.getenv("ANTHROPIC_API_KEY")
        self.openai_key = os.getenv("OPENAI_API_KEY")
        base_url = os.getenv("OPENAI_BASE_URL", "")
        
        # ä¼˜å…ˆçº§ï¼šOpenRouter > Claude > OpenAI > Ollama
        if self.openai_key and "openrouter.ai" in base_url:
            # OpenRouteré…ç½®
            self.api_key = self.openai_key
            self.base_url = base_url or "https://openrouter.ai/api/v1"
            self.model = os.getenv("OPENAI_MODEL", "xiaomi/mimo-v2-flash:free")
            self.provider = "openrouter"
        elif self.anthropic_key:
            # Claudeé…ç½®
            self.api_key = self.anthropic_key
            self.base_url = "https://api.anthropic.com"
            self.model = os.getenv("CLAUDE_MODEL", "claude-3-5-sonnet-20241022")
            self.provider = "claude"
        elif self.openai_key and base_url:
            # OpenAIæˆ–å…¶ä»–å…¼å®¹APIé…ç½®
            self.api_key = self.openai_key
            self.base_url = base_url
            self.model = os.getenv("OPENAI_MODEL", "gpt-3.5-turbo")
            if "localhost" in base_url or "127.0.0.1" in base_url:
                self.provider = "ollama"
            else:
                self.provider = "openai"
        elif self.openai_key:
            # é»˜è®¤OpenAIé…ç½®
            self.api_key = self.openai_key
            self.base_url = "https://api.openai.com/v1"
            self.model = os.getenv("OPENAI_MODEL", "gpt-3.5-turbo")
            self.provider = "openai"
        else:
            # æ— APIå¯†é’¥ï¼Œä»…ä½¿ç”¨é™çº§æ¨¡å¼
            self.api_key = None
            self.base_url = None
            self.model = None
            self.provider = "fallback_only"
        
        # é…ç½®å‚æ•°
        self.daily_limit = int(os.getenv("LLM_DAILY_LIMIT", "45"))
        self.cache_ttl = int(os.getenv("LLM_CACHE_TTL", "3600"))
        self.timeout = int(os.getenv("LLM_TIMEOUT", "10"))
        self.max_tokens = int(os.getenv("LLM_MAX_TOKENS", "500"))
        self.temperature = float(os.getenv("LLM_TEMPERATURE", "0.7"))
        
        # ä½¿ç”¨é™åˆ¶
        self.call_count = 0
        self.last_reset = datetime.now().date()
        
        # ç¼“å­˜æœºåˆ¶
        self.response_cache = {}
        
        # é™çº§å“åº”æ¨¡æ¿
        self.fallback_templates = {
            "intent_classification": self._classify_intent_fallback,
            "briefing_generation": self._generate_briefing_fallback,
            "response_enhancement": self._enhance_response_fallback,
            "entity_extraction": self._extract_entities_fallback
        }
        
        logger.info(f"LLMå®¢æˆ·ç«¯åˆå§‹åŒ– - æä¾›å•†: {self.provider}, æ¨¡å‹: {self.model}")
    
    def _reset_daily_counter(self):
        """é‡ç½®æ¯æ—¥è®¡æ•°å™¨"""
        today = datetime.now().date()
        if today > self.last_reset:
            self.call_count = 0
            self.last_reset = today
            logger.info("æ¯æ—¥LLMè°ƒç”¨è®¡æ•°å™¨å·²é‡ç½®")
    
    def can_use_llm(self) -> bool:
        """æ£€æŸ¥æ˜¯å¦å¯ä»¥ä½¿ç”¨LLM"""
        self._reset_daily_counter()
        
        if not self.api_key:
            return False
        
        if self.call_count >= self.daily_limit:
            logger.warning(f"å·²è¾¾åˆ°æ¯æ—¥LLMè°ƒç”¨é™åˆ¶: {self.call_count}/{self.daily_limit}")
            return False
        
        return True
    
    def _get_cache_key(self, task_type: str, content: str) -> str:
        """ç”Ÿæˆç¼“å­˜é”®"""
        return f"{task_type}:{hash(content)}"
    
    def _get_cached_response(self, cache_key: str) -> Optional[LLMResponse]:
        """è·å–ç¼“å­˜å“åº”"""
        if cache_key in self.response_cache:
            cached_item = self.response_cache[cache_key]
            
            # æ£€æŸ¥æ˜¯å¦è¿‡æœŸ
            if datetime.now() - cached_item["timestamp"] < timedelta(seconds=self.cache_ttl):
                logger.debug(f"LLMç¼“å­˜å‘½ä¸­: {cache_key}")
                return cached_item["response"]
            else:
                # æ¸…ç†è¿‡æœŸç¼“å­˜
                del self.response_cache[cache_key]
        
        return None
    
    def _cache_response(self, cache_key: str, response: LLMResponse):
        """ç¼“å­˜å“åº”"""
        self.response_cache[cache_key] = {
            "response": response,
            "timestamp": datetime.now()
        }
    
    async def process_with_fallback(self, task_type: str, content: str, 
                                  context: Dict[str, Any] = None) -> LLMResponse:
        """
        å¸¦é™çº§çš„LLMå¤„ç†
        
        Args:
            task_type: ä»»åŠ¡ç±»å‹ (intent_classification, briefing_generation, etc.)
            content: è¾“å…¥å†…å®¹
            context: ä¸Šä¸‹æ–‡ä¿¡æ¯
            
        Returns:
            LLMResponseå¯¹è±¡
        """
        # ç¡®ä¿ context ä¸ä¸º None
        if context is None:
            context = {}
            
        cache_key = self._get_cache_key(task_type, content)
        
        # 1. æ£€æŸ¥ç¼“å­˜
        cached = self._get_cached_response(cache_key)
        if cached:
            return cached
        
        # 2. å°è¯•LLMè°ƒç”¨
        if self.can_use_llm():
            try:
                response = await self._call_llm(task_type, content, context)
                if response.success:
                    self._cache_response(cache_key, response)
                    return response
            except Exception as e:
                logger.warning(f"LLMè°ƒç”¨å¤±è´¥ï¼Œé™çº§åˆ°è§„åˆ™å¼•æ“: {e}")
        
        # 3. é™çº§åˆ°è§„åˆ™å¼•æ“
        fallback_func = self.fallback_templates.get(task_type)
        if fallback_func:
            response = fallback_func(content, context)
            self._cache_response(cache_key, response)
            return response
        
        # 4. é»˜è®¤å“åº”
        return LLMResponse(
            content="æŠ±æ­‰ï¼Œæš‚æ—¶æ— æ³•å¤„ç†æ‚¨çš„è¯·æ±‚",
            success=False,
            source="fallback",
            error="No fallback handler available"
        )
    
    async def _call_llm(self, task_type: str, content: str, 
                       context: Dict[str, Any]) -> LLMResponse:
        """è°ƒç”¨LLM API"""
        start_time = datetime.now()
        
        # æ„å»ºæç¤ºè¯
        prompt = self._build_prompt(task_type, content, context)
        
        try:
            if self.provider == "claude":
                return await self._call_claude(task_type, prompt, start_time)
            else:
                return await self._call_openai_compatible(task_type, prompt, start_time)
        
        except Exception as e:
            response_time = (datetime.now() - start_time).total_seconds()
            return LLMResponse(
                content="",
                success=False,
                source="llm",
                response_time=response_time,
                error=str(e)
            )
    
    async def _call_claude(self, task_type: str, prompt: str, start_time: datetime) -> LLMResponse:
        """è°ƒç”¨Claude API"""
        headers = {
            "x-api-key": self.api_key,
            "Content-Type": "application/json",
            "anthropic-version": "2023-06-01"
        }
        
        payload = {
            "model": self.model,
            "max_tokens": self.max_tokens,
            "messages": [
                {
                    "role": "user", 
                    "content": f"{self._get_system_prompt(task_type)}\n\n{prompt}"
                }
            ]
        }
        
        async with aiohttp.ClientSession() as session:
            async with session.post(
                f"{self.base_url}/v1/messages",
                headers=headers,
                json=payload,
                timeout=aiohttp.ClientTimeout(total=self.timeout)
            ) as resp:
                if resp.status == 200:
                    data = await resp.json()
                    
                    # æ›´æ–°è°ƒç”¨è®¡æ•°
                    self.call_count += 1
                    
                    response_time = (datetime.now() - start_time).total_seconds()
                    
                    return LLMResponse(
                        content=data["content"][0]["text"],
                        success=True,
                        source="llm",
                        tokens_used=data.get("usage", {}).get("output_tokens", 0),
                        response_time=response_time
                    )
                else:
                    error_text = await resp.text()
                    raise Exception(f"Claude APIé”™è¯¯ {resp.status}: {error_text}")
    
    async def _call_openai_compatible(self, task_type: str, prompt: str, start_time: datetime) -> LLMResponse:
        """è°ƒç”¨OpenAIå…¼å®¹APIï¼ˆOpenRouterã€OpenAIã€Ollamaç­‰ï¼‰"""
        headers = {
            "Authorization": f"Bearer {self.api_key}",
            "Content-Type": "application/json"
        }
        
        # OpenRouteréœ€è¦é¢å¤–çš„å¤´éƒ¨
        if self.provider == "openrouter":
            headers.update({
                "HTTP-Referer": "https://github.com/yourusername/yougame-explorer",
                "X-Title": "YouGame Explorer"
            })
        
        payload = {
            "model": self.model,
            "messages": [
                {"role": "system", "content": self._get_system_prompt(task_type)},
                {"role": "user", "content": prompt}
            ],
            "max_tokens": self.max_tokens,
            "temperature": self.temperature
        }
        
        async with aiohttp.ClientSession() as session:
            async with session.post(
                f"{self.base_url}/chat/completions",
                headers=headers,
                json=payload,
                timeout=aiohttp.ClientTimeout(total=self.timeout)
            ) as resp:
                if resp.status == 200:
                    data = await resp.json()
                    
                    # æ›´æ–°è°ƒç”¨è®¡æ•°
                    self.call_count += 1
                    
                    response_time = (datetime.now() - start_time).total_seconds()
                    
                    return LLMResponse(
                        content=data["choices"][0]["message"]["content"],
                        success=True,
                        source="llm",
                        tokens_used=data.get("usage", {}).get("total_tokens", 0),
                        response_time=response_time
                    )
                else:
                    error_text = await resp.text()
                    raise Exception(f"APIé”™è¯¯ {resp.status}: {error_text}")
    
    def _build_prompt(self, task_type: str, content: str, context: Dict[str, Any]) -> str:
        """æ„å»ºä»»åŠ¡ç‰¹å®šçš„æç¤ºè¯"""
        prompts = {
            "intent_classification": f"""
åˆ†æç”¨æˆ·æŸ¥è¯¢çš„æ„å›¾ï¼Œä»ä»¥ä¸‹ç±»åˆ«ä¸­é€‰æ‹©æœ€åˆé€‚çš„ï¼š
- ç›´æ’­æŸ¥è¯¢ï¼šæŸ¥è¯¢ä¸»æ’­ç›´æ’­çŠ¶æ€
- ç®€æŠ¥ç”Ÿæˆï¼šç”Ÿæˆæ¸¸æˆåœˆç®€æŠ¥
- æ•°æ®åˆ†æï¼šåˆ†ææ¸¸æˆæ•°æ®è¶‹åŠ¿
- é—®å€™ï¼šæ‰“æ‹›å‘¼æˆ–é—²èŠ
- å…¶ä»–ï¼šæ— æ³•åˆ†ç±»çš„æŸ¥è¯¢

ç”¨æˆ·æŸ¥è¯¢ï¼š"{content}"

è¯·è¿”å›JSONæ ¼å¼ï¼š
{{"intent": "æ„å›¾ç±»åˆ«", "confidence": 0.95, "entities": {{"ä¸»æ’­å": "Faker"}}}}
""",
            
            "briefing_generation": f"""
åŸºäºä»¥ä¸‹æ¸¸æˆç›´æ’­æ•°æ®ï¼Œç”Ÿæˆä¸€ä»½ç®€æ´æœ‰è¶£çš„æ¸¸æˆåœˆç®€æŠ¥ï¼š

æ•°æ®ï¼š{context.get('data', {})}

è¦æ±‚ï¼š
1. ä½¿ç”¨emojiå¢åŠ è¶£å‘³æ€§
2. çªå‡ºé‡ç‚¹æ•°æ®å’Œè¶‹åŠ¿
3. ä¿æŒç®€æ´ï¼Œä¸è¶…è¿‡200å­—
4. ä½¿ç”¨ä¸­æ–‡

è¯·ç”Ÿæˆç®€æŠ¥å†…å®¹ï¼š
""",
            
            "response_enhancement": f"""
ä¼˜åŒ–ä»¥ä¸‹å›å¤ï¼Œä½¿å…¶æ›´åŠ ç”ŸåŠ¨æœ‰è¶£ï¼š

åŸå§‹å›å¤ï¼š"{content}"
ä¸Šä¸‹æ–‡ï¼š{context}

è¦æ±‚ï¼š
1. ä¿æŒåŸæ„ä¸å˜
2. æ·»åŠ åˆé€‚çš„emoji
3. ä½¿ç”¨æ›´ç”ŸåŠ¨çš„è¡¨è¾¾
4. ä¿æŒä¸“ä¸šæ€§

ä¼˜åŒ–åçš„å›å¤ï¼š
""",
            
            "entity_extraction": f"""
ä»ç”¨æˆ·æŸ¥è¯¢ä¸­æå–å…³é”®å®ä½“ï¼š

æŸ¥è¯¢ï¼š"{content}"

è¯·æå–ï¼š
- ä¸»æ’­åç§°
- æ¸¸æˆåç§°  
- æ—¶é—´èŒƒå›´
- å¹³å°åç§°

è¿”å›JSONæ ¼å¼ï¼š
{{"ä¸»æ’­å": "Faker", "æ¸¸æˆ": "è‹±é›„è”ç›Ÿ", "æ—¶é—´": "ä»Šå¤©", "å¹³å°": "è™ç‰™"}}
"""
        }
        
        return prompts.get(task_type, content)
    
    def _get_system_prompt(self, task_type: str) -> str:
        """è·å–ç³»ç»Ÿæç¤ºè¯"""
        return """ä½ æ˜¯å°æ¸¸æ¢AIåŠ©æ‰‹ï¼Œä¸“é—¨å¤„ç†æ¸¸æˆåœˆç›¸å…³æŸ¥è¯¢ã€‚
è¯·æ ¹æ®ç”¨æˆ·éœ€æ±‚æä¾›å‡†ç¡®ã€æœ‰ç”¨çš„å›å¤ã€‚
ä¿æŒå›å¤ç®€æ´æ˜äº†ï¼Œä½¿ç”¨ä¸­æ–‡å›å¤ã€‚"""
    
    # é™çº§å¤„ç†å‡½æ•°
    def _classify_intent_fallback(self, content: str, context: Dict) -> LLMResponse:
        """æ„å›¾åˆ†ç±»é™çº§å¤„ç†"""
        content_lower = content.lower()
        
        # è§„åˆ™åŒ¹é…
        if any(word in content_lower for word in ["ç›´æ’­", "å¼€æ’­", "åœ¨çº¿", "live"]):
            intent = "ç›´æ’­æŸ¥è¯¢"
            confidence = 0.9
        elif any(word in content_lower for word in ["ç®€æŠ¥", "æ±‡æ€»", "æŠ¥å‘Š", "briefing"]):
            intent = "ç®€æŠ¥ç”Ÿæˆ"
            confidence = 0.9
        elif any(word in content_lower for word in ["ä½ å¥½", "å—¨", "hello", "hi"]):
            intent = "é—®å€™"
            confidence = 0.95
        else:
            intent = "å…¶ä»–"
            confidence = 0.6
        
        # ç®€å•å®ä½“æå–
        entities = {}
        known_streamers = ["Faker", "Uzi", "å¤§å¸é©¬", "TheShy", "Rookie"]
        for streamer in known_streamers:
            if streamer in content:
                entities["ä¸»æ’­å"] = streamer
                break
        
        result = {
            "intent": intent,
            "confidence": confidence,
            "entities": entities
        }
        
        return LLMResponse(
            content=json.dumps(result, ensure_ascii=False),
            success=True,
            source="fallback"
        )
    
    def _generate_briefing_fallback(self, content: str, context: Dict) -> LLMResponse:
        """ç®€æŠ¥ç”Ÿæˆé™çº§å¤„ç†"""
        data = context.get('data', {})
        live_count = len(data.get('live_streams', []))
        
        briefing = f"""ğŸ“° ã€å°æ¸¸æ¢ç®€æŠ¥ã€‘
        
ğŸ”¥ å½“å‰ç›´æ’­: {live_count}ä½ä¸»æ’­åœ¨çº¿
ğŸ“Š ç³»ç»Ÿè¿è¡Œæ­£å¸¸ï¼Œæ•°æ®æ›´æ–°åŠæ—¶
ğŸ® çƒ­é—¨æ¸¸æˆ: è‹±é›„è”ç›Ÿã€ç‹è€…è£è€€æŒç»­ç«çƒ­

ğŸ’¡ æ›´å¤šè¯¦æƒ…è¯·æŸ¥è¯¢å…·ä½“ä¸»æ’­çŠ¶æ€"""
        
        return LLMResponse(
            content=briefing,
            success=True,
            source="fallback"
        )
    
    def _enhance_response_fallback(self, content: str, context: Dict) -> LLMResponse:
        """å“åº”ä¼˜åŒ–é™çº§å¤„ç†"""
        # ç®€å•çš„emojiæ·»åŠ 
        enhanced = content
        if "ç›´æ’­" in content:
            enhanced = f"ğŸ”´ {enhanced}"
        if "è§‚ä¼—" in content or "äººæ°”" in content:
            enhanced = enhanced.replace("è§‚ä¼—", "ğŸ‘¥è§‚ä¼—").replace("äººæ°”", "ğŸ‘¥äººæ°”")
        
        return LLMResponse(
            content=enhanced,
            success=True,
            source="fallback"
        )
    
    def _extract_entities_fallback(self, content: str, context: Dict) -> LLMResponse:
        """å®ä½“æå–é™çº§å¤„ç†"""
        entities = {}
        
        # ä¸»æ’­åæå–
        known_streamers = ["Faker", "Uzi", "å¤§å¸é©¬", "TheShy", "Rookie", "PDD", "å°å›¢å›¢"]
        for streamer in known_streamers:
            if streamer in content:
                entities["ä¸»æ’­å"] = streamer
                break
        
        # æ—¶é—´æå–
        if "ä»Šå¤©" in content:
            entities["æ—¶é—´"] = "ä»Šå¤©"
        elif "æœ€è¿‘" in content:
            entities["æ—¶é—´"] = "æœ€è¿‘"
        
        # å¹³å°æå–
        if "è™ç‰™" in content:
            entities["å¹³å°"] = "è™ç‰™"
        elif "twitch" in content.lower():
            entities["å¹³å°"] = "Twitch"
        
        return LLMResponse(
            content=json.dumps(entities, ensure_ascii=False),
            success=True,
            source="fallback"
        )
    
    def get_usage_stats(self) -> Dict[str, Any]:
        """è·å–ä½¿ç”¨ç»Ÿè®¡"""
        return {
            "provider": self.provider,
            "model": self.model,
            "daily_calls": self.call_count,
            "daily_limit": self.daily_limit,
            "remaining_calls": max(0, self.daily_limit - self.call_count),
            "cache_size": len(self.response_cache),
            "last_reset": self.last_reset.isoformat(),
            "api_configured": self.api_key is not None,
            "fallback_only": self.provider == "fallback_only"
        }

# å…¨å±€å®ä¾‹
llm_client = LLMClient()

# æµ‹è¯•ä»£ç 
async def test_llm_client():
    """æµ‹è¯•LLMå®¢æˆ·ç«¯"""
    print("ğŸ§ª æµ‹è¯•LLMå®¢æˆ·ç«¯...")
    
    # æµ‹è¯•æ„å›¾åˆ†ç±»
    print("\n1. æµ‹è¯•æ„å›¾åˆ†ç±»:")
    result = await llm_client.process_with_fallback(
        "intent_classification", 
        "Fakeråœ¨ç›´æ’­å—ï¼Ÿ"
    )
    print(f"   ç»“æœ: {result.content}")
    print(f"   æ¥æº: {result.source}")
    
    # æµ‹è¯•ç®€æŠ¥ç”Ÿæˆ
    print("\n2. æµ‹è¯•ç®€æŠ¥ç”Ÿæˆ:")
    context = {
        "data": {
            "live_streams": [
                {"user_name": "Faker", "viewer_count": 45000},
                {"user_name": "Uzi", "viewer_count": 30000}
            ]
        }
    }
    result = await llm_client.process_with_fallback(
        "briefing_generation",
        "ç”Ÿæˆç®€æŠ¥",
        context
    )
    print(f"   ç»“æœ: {result.content}")
    print(f"   æ¥æº: {result.source}")
    
    # æ˜¾ç¤ºä½¿ç”¨ç»Ÿè®¡
    print(f"\nğŸ“Š ä½¿ç”¨ç»Ÿè®¡: {llm_client.get_usage_stats()}")

if __name__ == "__main__":
    asyncio.run(test_llm_client())