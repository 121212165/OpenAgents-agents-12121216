#!/usr/bin/env python3
"""
LLMé…ç½®æ£€æŸ¥è„šæœ¬
æ£€æŸ¥å¤§æ¨¡å‹é…ç½®æ˜¯å¦æ­£ç¡®ï¼Œå¹¶æä¾›é…ç½®å»ºè®®
"""

import os
import sys
import asyncio
from pathlib import Path

# æ·»åŠ é¡¹ç›®æ ¹ç›®å½•åˆ°è·¯å¾„
project_root = Path(__file__).parent.parent
sys.path.insert(0, str(project_root))

from src.utils.llm_client import LLMClient
from loguru import logger

def check_env_file():
    """æ£€æŸ¥ç¯å¢ƒå˜é‡æ–‡ä»¶"""
    print("ğŸ” æ£€æŸ¥ç¯å¢ƒå˜é‡é…ç½®...")
    
    env_file = project_root / ".env"
    env_example = project_root / ".env.example"
    
    if not env_file.exists():
        print("âŒ æœªæ‰¾åˆ° .env æ–‡ä»¶")
        if env_example.exists():
            print("ğŸ’¡ å»ºè®®ï¼šå¤åˆ¶ .env.example ä¸º .env å¹¶é…ç½®APIå¯†é’¥")
            print(f"   cp {env_example} {env_file}")
        return False
    
    print("âœ… æ‰¾åˆ° .env æ–‡ä»¶")
    return True

def check_api_keys():
    """æ£€æŸ¥APIå¯†é’¥é…ç½®"""
    print("\nğŸ”‘ æ£€æŸ¥APIå¯†é’¥é…ç½®...")
    
    # æ£€æŸ¥å„ç§APIå¯†é’¥
    openai_key = os.getenv("OPENAI_API_KEY")
    anthropic_key = os.getenv("ANTHROPIC_API_KEY")
    openai_base = os.getenv("OPENAI_BASE_URL", "")
    
    configs = []
    
    if openai_key and "openrouter.ai" in openai_base:
        configs.append("OpenRouter")
        print("âœ… æ£€æµ‹åˆ° OpenRouter é…ç½®")
    elif anthropic_key:
        configs.append("Claude")
        print("âœ… æ£€æµ‹åˆ° Claude é…ç½®")
    elif openai_key and "api.openai.com" in openai_base:
        configs.append("OpenAI")
        print("âœ… æ£€æµ‹åˆ° OpenAI é…ç½®")
    elif openai_key and "localhost" in openai_base:
        configs.append("Ollama")
        print("âœ… æ£€æµ‹åˆ° Ollama é…ç½®")
    
    if not configs:
        print("âŒ æœªæ£€æµ‹åˆ°æœ‰æ•ˆçš„APIé…ç½®")
        print("\nğŸ’¡ é…ç½®å»ºè®®ï¼š")
        print("1. OpenRouterï¼ˆæ¨èï¼Œæœ‰å…è´¹é¢åº¦ï¼‰ï¼š")
        print("   - æ³¨å†Œï¼šhttps://openrouter.ai/")
        print("   - è®¾ç½® OPENAI_API_KEY=your_openrouter_key")
        print("   - è®¾ç½® OPENAI_BASE_URL=https://openrouter.ai/api/v1")
        print("   - è®¾ç½® OPENAI_MODEL=xiaomi/mimo-v2-flash:free")
        print("\n2. æœ¬åœ°Ollamaï¼ˆå®Œå…¨å…è´¹ï¼‰ï¼š")
        print("   - å®‰è£…ï¼šhttps://ollama.ai/")
        print("   - è¿è¡Œï¼šollama serve")
        print("   - è®¾ç½® OPENAI_BASE_URL=http://localhost:11434/v1")
        print("   - è®¾ç½® OPENAI_MODEL=llama3.2:3b")
        return False
    
    return True

async def test_llm_connection():
    """æµ‹è¯•LLMè¿æ¥"""
    print("\nğŸ§ª æµ‹è¯•LLMè¿æ¥...")
    
    try:
        client = LLMClient()
        stats = client.get_usage_stats()
        
        print(f"ğŸ“Š LLMçŠ¶æ€ï¼š")
        print(f"   æä¾›å•†: {stats['provider']}")
        print(f"   æ¨¡å‹: {stats['model']}")
        print(f"   APIé…ç½®: {'âœ…' if stats['api_configured'] else 'âŒ'}")
        print(f"   ä»…é™çº§æ¨¡å¼: {'æ˜¯' if stats['fallback_only'] else 'å¦'}")
        
        if not stats['api_configured']:
            print("âš ï¸  æœªé…ç½®APIå¯†é’¥ï¼Œå°†ä½¿ç”¨é™çº§æ¨¡å¼")
            return True  # é™çº§æ¨¡å¼ä¹Ÿæ˜¯å¯ä»¥å·¥ä½œçš„
        
        # æµ‹è¯•ç®€å•è°ƒç”¨
        print("\nğŸ”„ æµ‹è¯•APIè°ƒç”¨...")
        result = await client.process_with_fallback(
            "intent_classification",
            "ä½ å¥½"
        )
        
        if result.success:
            print(f"âœ… APIè°ƒç”¨æˆåŠŸ")
            print(f"   å“åº”æ¥æº: {result.source}")
            print(f"   å“åº”æ—¶é—´: {result.response_time:.2f}s")
            if result.tokens_used > 0:
                print(f"   ä½¿ç”¨Token: {result.tokens_used}")
        else:
            print(f"âŒ APIè°ƒç”¨å¤±è´¥: {result.error}")
            print("ğŸ’¡ å°†ä½¿ç”¨é™çº§æ¨¡å¼ï¼ŒåŠŸèƒ½æœ‰é™ä½†å¯æ­£å¸¸è¿è¡Œ")
        
        return True
        
    except Exception as e:
        print(f"âŒ LLMå®¢æˆ·ç«¯åˆå§‹åŒ–å¤±è´¥: {e}")
        return False

def print_configuration_guide():
    """æ‰“å°é…ç½®æŒ‡å—"""
    print("\nğŸ“– é…ç½®æŒ‡å—ï¼š")
    print("\n1. å…è´¹æ–¹æ¡ˆï¼ˆæ¨èæ–°æ‰‹ï¼‰ï¼š")
    print("   - ä½¿ç”¨OpenRouterå…è´¹æ¨¡å‹")
    print("   - æ¯æ—¥æœ‰ä¸€å®šå…è´¹é¢åº¦")
    print("   - æ³¨å†Œç®€å•ï¼Œå³ç”¨å³å¾—")
    
    print("\n2. æœ¬åœ°æ–¹æ¡ˆï¼ˆæ¨èå¼€å‘è€…ï¼‰ï¼š")
    print("   - å®‰è£…Ollamaè¿è¡Œæœ¬åœ°æ¨¡å‹")
    print("   - å®Œå…¨å…è´¹ï¼Œæ— é™åˆ¶")
    print("   - éœ€è¦ä¸€å®šç¡¬ä»¶èµ„æº")
    
    print("\n3. ä»˜è´¹æ–¹æ¡ˆï¼ˆæ¨èç”Ÿäº§ç¯å¢ƒï¼‰ï¼š")
    print("   - OpenAI GPTç³»åˆ—")
    print("   - Claudeç³»åˆ—")
    print("   - è´¨é‡æœ€é«˜ï¼Œé€Ÿåº¦æœ€å¿«")
    
    print("\n4. é™çº§æ–¹æ¡ˆï¼ˆä¿åº•ï¼‰ï¼š")
    print("   - ä¸é…ç½®ä»»ä½•API")
    print("   - ä½¿ç”¨å†…ç½®è§„åˆ™å¼•æ“")
    print("   - åŠŸèƒ½æœ‰é™ä½†å¯è¿è¡Œ")

async def main():
    """ä¸»å‡½æ•°"""
    print("ğŸš€ å°æ¸¸æ¢ LLMé…ç½®æ£€æŸ¥å·¥å…·")
    print("=" * 50)
    
    # æ£€æŸ¥ç¯å¢ƒæ–‡ä»¶
    env_ok = check_env_file()
    
    # æ£€æŸ¥APIå¯†é’¥
    api_ok = check_api_keys()
    
    # æµ‹è¯•è¿æ¥
    connection_ok = await test_llm_connection()
    
    print("\n" + "=" * 50)
    print("ğŸ“‹ æ£€æŸ¥ç»“æœæ±‡æ€»ï¼š")
    print(f"   ç¯å¢ƒæ–‡ä»¶: {'âœ…' if env_ok else 'âŒ'}")
    print(f"   APIé…ç½®: {'âœ…' if api_ok else 'âŒ'}")
    print(f"   è¿æ¥æµ‹è¯•: {'âœ…' if connection_ok else 'âŒ'}")
    
    if env_ok and connection_ok:
        print("\nğŸ‰ é…ç½®æ£€æŸ¥å®Œæˆï¼ç³»ç»Ÿå¯ä»¥æ­£å¸¸è¿è¡Œã€‚")
        if not api_ok:
            print("âš ï¸  æ³¨æ„ï¼šå½“å‰ä½¿ç”¨é™çº§æ¨¡å¼ï¼Œå»ºè®®é…ç½®APIä»¥è·å¾—æ›´å¥½ä½“éªŒã€‚")
    else:
        print("\nâŒ é…ç½®å­˜åœ¨é—®é¢˜ï¼Œè¯·å‚è€ƒä¸Šè¿°å»ºè®®è¿›è¡Œä¿®å¤ã€‚")
        print_configuration_guide()
    
    return env_ok and connection_ok

if __name__ == "__main__":
    # åŠ è½½ç¯å¢ƒå˜é‡
    from dotenv import load_dotenv
    load_dotenv()
    
    # è¿è¡Œæ£€æŸ¥
    success = asyncio.run(main())
    sys.exit(0 if success else 1)